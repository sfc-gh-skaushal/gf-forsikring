{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîç Fraud Detection Model with Snowpark ML\n",
        "\n",
        "## InsuranceCo - Snowflake Horizon Demo\n",
        "\n",
        "This notebook demonstrates how Data Scientists can build ML models directly on governed data in Snowflake using Snowpark. Key points:\n",
        "\n",
        "- **Data stays in Snowflake** - no data movement required\n",
        "- **Governance policies apply** - PII is accessible only to DATA_SCIENTIST role\n",
        "- **Full lineage tracking** - model inputs are traced back to source\n",
        "- **Scalable compute** - leverage Snowflake warehouses for training\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from snowflake.snowpark import Session\n",
        "from snowflake.snowpark.functions import col, when, lit, avg, sum as sum_, count, corr\n",
        "from snowflake.snowpark.types import FloatType, IntegerType, StringType\n",
        "from snowflake.ml.modeling.preprocessing import StandardScaler, OneHotEncoder\n",
        "from snowflake.ml.modeling.ensemble import RandomForestClassifier\n",
        "from snowflake.ml.modeling.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Snowpark session\n",
        "# When running in Snowsight Notebooks, session is automatically available via get_active_session()\n",
        "\n",
        "# For Snowsight notebooks (recommended):\n",
        "session = get_active_session()\n",
        "\n",
        "# For local development, uncomment and configure:\n",
        "# connection_parameters = {\n",
        "#     \"account\": \"<your-account>\",\n",
        "#     \"user\": \"<your-username>\",\n",
        "#     \"password\": \"<your-password>\",  # Use key-pair auth in production\n",
        "#     \"role\": \"DATA_SCIENTIST\",\n",
        "#     \"warehouse\": \"INSURANCECO_ML_WH\",\n",
        "#     \"database\": \"INSURANCECO\",\n",
        "#     \"schema\": \"DATA_SCIENCE\"\n",
        "# }\n",
        "# session = Session.builder.configs(connection_parameters).create()\n",
        "\n",
        "print(f\"‚úÖ Connected to Snowflake\")\n",
        "print(f\"   Role: {session.get_current_role()}\")\n",
        "print(f\"   Warehouse: {session.get_current_warehouse()}\")\n",
        "print(f\"   Database: {session.get_current_database()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Governed Data\n",
        "\n",
        "We load data from the curated `DIM_CLAIMS` table. As a DATA_SCIENTIST, we have full access to PII fields (required for fraud pattern analysis). Other roles would see masked data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load claims data from curated layer\n",
        "claims_df = session.table(\"INSURANCECO.CURATED.DIM_CLAIMS\")\n",
        "\n",
        "# Load policy data for enrichment\n",
        "policies_df = session.table(\"INSURANCECO.CURATED.DIM_POLICIES\")\n",
        "\n",
        "print(f\"üìä Loaded {claims_df.count()} claims records\")\n",
        "print(f\"üìä Loaded {policies_df.count()} policy records\")\n",
        "\n",
        "# Preview the data (PII visible because we're DATA_SCIENTIST)\n",
        "print(\"\\nüîì DATA_SCIENTIST role can see full PII:\")\n",
        "claims_df.select(\n",
        "    \"CLAIM_ID\", \n",
        "    \"POLICY_HOLDER_NAME\",  # PII - visible to DATA_SCIENTIST\n",
        "    \"POLICY_HOLDER_EMAIL\", # PII - visible to DATA_SCIENTIST  \n",
        "    \"CLAIM_AMOUNT\",\n",
        "    \"FRAUD_FLAG\"\n",
        ").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze fraud distribution\n",
        "fraud_dist = claims_df.group_by(\"FRAUD_FLAG\").agg(\n",
        "    count(\"*\").alias(\"COUNT\"),\n",
        "    avg(\"CLAIM_AMOUNT\").alias(\"AVG_CLAIM_AMOUNT\"),\n",
        "    avg(\"COVERAGE_UTILIZATION_PCT\").alias(\"AVG_COVERAGE_UTIL\")\n",
        ")\n",
        "\n",
        "print(\"üìä Fraud vs Non-Fraud Distribution:\")\n",
        "fraud_dist.show()\n",
        "\n",
        "# Analyze claims that exceed coverage (potential fraud indicator)\n",
        "exceeds_coverage = claims_df.filter(col(\"EXCEEDS_COVERAGE\") == True)\n",
        "print(f\"\\n‚ö†Ô∏è Claims exceeding coverage: {exceeds_coverage.count()}\")\n",
        "print(\"\\nüìã Details of claims exceeding coverage:\")\n",
        "exceeds_coverage.select(\n",
        "    \"CLAIM_ID\",\n",
        "    \"CLAIM_AMOUNT\",\n",
        "    \"POLICY_COVERAGE_LIMIT\",\n",
        "    \"COVERAGE_UTILIZATION_PCT\",\n",
        "    \"FRAUD_FLAG\",\n",
        "    \"ADJUSTER_NOTES\"\n",
        ").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Engineering\n",
        "\n",
        "Create features for the fraud detection model using Snowpark transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Join claims with policies for enriched features\n",
        "enriched_df = claims_df.join(\n",
        "    policies_df.select(\n",
        "        \"POLICY_ID\",\n",
        "        col(\"RISK_SCORE\").alias(\"POLICY_RISK_SCORE\"),\n",
        "        col(\"PREVIOUS_CLAIMS_COUNT\").alias(\"PREV_CLAIMS\"),\n",
        "        col(\"DRIVER_AGE\").alias(\"DRIVER_AGE\"),\n",
        "        col(\"YEARS_LICENSED\").alias(\"YEARS_LICENSED\"),\n",
        "        col(\"POLICY_TYPE\").alias(\"POLICY_TYPE\"),\n",
        "        col(\"PREMIUM_ANNUAL\").alias(\"PREMIUM\")\n",
        "    ),\n",
        "    on=\"POLICY_ID\"\n",
        ")\n",
        "\n",
        "# Create derived features\n",
        "feature_df = enriched_df.select(\n",
        "    col(\"CLAIM_ID\"),\n",
        "    col(\"FRAUD_FLAG\").cast(IntegerType()).alias(\"IS_FRAUD\"),\n",
        "    col(\"CLAIM_AMOUNT\"),\n",
        "    col(\"COVERAGE_UTILIZATION_PCT\"),\n",
        "    col(\"DAYS_TO_REPORT\"),\n",
        "    col(\"CLAIM_TYPE\"),\n",
        "    col(\"EXCEEDS_COVERAGE\").cast(IntegerType()).alias(\"EXCEEDS_COVERAGE\"),\n",
        "    col(\"HIGH_VALUE_CLAIM\").cast(IntegerType()).alias(\"HIGH_VALUE\"),\n",
        "    col(\"VEHICLE_AGE\"),\n",
        "    col(\"POLICY_RISK_SCORE\"),\n",
        "    col(\"PREV_CLAIMS\"),\n",
        "    col(\"DRIVER_AGE\"),\n",
        "    col(\"YEARS_LICENSED\"),\n",
        "    col(\"POLICY_TYPE\"),\n",
        "    (col(\"CLAIM_AMOUNT\") / col(\"PREMIUM\")).alias(\"CLAIM_PREMIUM_RATIO\")\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Created feature DataFrame with {len(feature_df.columns)} columns\")\n",
        "feature_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Prepare Training Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define feature columns and target\n",
        "numeric_features = [\n",
        "    \"CLAIM_AMOUNT\", \"COVERAGE_UTILIZATION_PCT\", \"DAYS_TO_REPORT\",\n",
        "    \"EXCEEDS_COVERAGE\", \"HIGH_VALUE\", \"VEHICLE_AGE\",\n",
        "    \"PREV_CLAIMS\", \"DRIVER_AGE\", \"YEARS_LICENSED\", \"CLAIM_PREMIUM_RATIO\"\n",
        "]\n",
        "categorical_features = [\"CLAIM_TYPE\", \"POLICY_RISK_SCORE\", \"POLICY_TYPE\"]\n",
        "target = \"IS_FRAUD\"\n",
        "\n",
        "print(f\"üìä Numeric features: {len(numeric_features)}\")\n",
        "print(f\"üìä Categorical features: {len(categorical_features)}\")\n",
        "print(f\"üéØ Target: {target}\")\n",
        "\n",
        "# Encode categorical variables using Snowflake ML\n",
        "encoder = OneHotEncoder(\n",
        "    input_cols=categorical_features,\n",
        "    output_cols=[f\"{c}_ENCODED\" for c in categorical_features],\n",
        "    drop_input_cols=True\n",
        ")\n",
        "encoded_df = encoder.fit(feature_df).transform(feature_df)\n",
        "\n",
        "# Scale numeric features\n",
        "scaler = StandardScaler(\n",
        "    input_cols=numeric_features,\n",
        "    output_cols=[f\"{c}_SCALED\" for c in numeric_features]\n",
        ")\n",
        "scaled_df = scaler.fit(encoded_df).transform(encoded_df)\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_df, test_df = scaled_df.random_split([0.8, 0.2], seed=42)\n",
        "print(f\"\\nüìä Training set: {train_df.count()} records\")\n",
        "print(f\"üìä Test set: {test_df.count()} records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train Fraud Detection Model\n",
        "\n",
        "Using Snowflake ML's RandomForestClassifier - training happens entirely within Snowflake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all feature columns (scaled numeric + encoded categorical)\n",
        "feature_cols = (\n",
        "    [f\"{c}_SCALED\" for c in numeric_features] + \n",
        "    [c for c in scaled_df.columns if \"_ENCODED\" in c and c != target]\n",
        ")\n",
        "\n",
        "print(f\"üî¢ Total features for model: {len(feature_cols)}\")\n",
        "\n",
        "# Initialize and train Random Forest model\n",
        "rf_model = RandomForestClassifier(\n",
        "    input_cols=feature_cols,\n",
        "    label_cols=[target],\n",
        "    output_cols=[\"PREDICTION\"],\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"üöÄ Training Random Forest model...\")\n",
        "print(\"   (All computation happens in Snowflake - data never leaves!)\")\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(train_df)\n",
        "print(\"‚úÖ Model training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
